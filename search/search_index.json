{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SF_biocontainer SF_biocontainer is a github repo with the Dockerfile used to build containers for CCRSF pipelines. Publicly available images Many of the popular tools have been containerized by the community, including Biocontainer community and different institutes. These resources can be found either on Docker hub or Quay CCRSF maintained images There are two scenarios that we need to build our own images: If the tool we want to use is not containerized before, we can build the container ourselves and deposite the image on Docker hub. For a particular task, multiple packages are required, so a customized environment is needed. For example, for analysis of single cell RNA-seq data, multiple R packages are required (Seurat, SingleR, etc). We can build an image with all the required packages: https://github.com/CCRSF-IFX/SF_biocontainer/blob/main/dockerhub_repos/sc-smk-wl/vr1.0.0/Dockerfile Here is a diagram to show the current workflow used for containerization: CCRSFIFX maintained images can be found here","title":"Containerization"},{"location":"#sf_biocontainer","text":"SF_biocontainer is a github repo with the Dockerfile used to build containers for CCRSF pipelines.","title":"SF_biocontainer"},{"location":"#publicly-available-images","text":"Many of the popular tools have been containerized by the community, including Biocontainer community and different institutes. These resources can be found either on Docker hub or Quay","title":"Publicly available images"},{"location":"#ccrsf-maintained-images","text":"There are two scenarios that we need to build our own images: If the tool we want to use is not containerized before, we can build the container ourselves and deposite the image on Docker hub. For a particular task, multiple packages are required, so a customized environment is needed. For example, for analysis of single cell RNA-seq data, multiple R packages are required (Seurat, SingleR, etc). We can build an image with all the required packages: https://github.com/CCRSF-IFX/SF_biocontainer/blob/main/dockerhub_repos/sc-smk-wl/vr1.0.0/Dockerfile Here is a diagram to show the current workflow used for containerization: CCRSFIFX maintained images can be found here","title":"CCRSF maintained images"},{"location":"dev_w_docker/","text":"When developing R scripts for single-cell sequencing data analysis, utilizing Docker containers offers a streamlined and reproducible environment. This documentation outlines the process of developing R scripts within a Docker container, leveraging JupyterLab for efficient coding and visualization. If the goal is to develop an R script named sc_singleR.prod.R , we create two files: sc_seurat_opt.R # https://github.com/CCRSF-IFX/SF_sc-smk-wl/blob/main/scripts/rna/sc_singleR_opt.R sc_singleR.ipynb # https://github.com/CCRSF-IFX/SF_sc-smk-wl/blob/main/scripts/rna/sc_singleR.ipynb Record the script path: script_path = getwd() # %exclude_jupyterlab% script_path # %exclude_jupyterlab% Invoke sc_singleR_opt.R for command line options Inside the notebook, we use the cell below to utilize sc_seurat_opt.R to read the command line options and save the information in an RDS file named opt.rds . system(paste0('Rscript ', script_path, # %exclude_jupyterlab% '/sc_singleR_opt.R --genome=\"hg38\" --markerList=\"/Volumes/ccrsf-ifx/Software/scripts/bin/currentsnake/single_cell/gene_lists/human_gene_list.csv\" --outdir=\"/Volumes/ccrsf-static/Analysis/xies4/github_repos/pipeline_dev_test/singleR\" --rds=\"/Volumes/ccrsf-static/Analysis/xies4/github_repos/pipeline_dev_test/test_dir/seur_10x_cluster_object.rds\"'), # %exclude_jupyterlab% intern = T) # %exclude_jupyterlab% Write the main code After capturing the options, read them using readRDS() and proceed with the main code: opt = readRDS(\"/Volumes/ccrsf-static/Analysis/xies4/github_repos/pipeline_dev_test/singleR/opt.rds\") # %exclude_jupyterlab% # main code goes here Generate production script: sc_singleR.prod.R Combine the necessary scripts and exclude irrelevant lines marked with exclude_jupyterlab to create the production script: notebook_prefix = \"sc_singleR\" # %exclude_jupyterlab% notebook_name = paste0(notebook_prefix, \".ipynb\") # %exclude_jupyterlab% notebook_r = paste0(script_path, \"/\", paste0(notebook_prefix, \".r\")) # %exclude_jupyterlab% notebook_path = paste0(script_path, \"/\", notebook_name) # %exclude_jupyterlab% opt_name = paste0(script_path, \"/\", sub(\".ipynb\", \"_opt.R\", notebook_name)) # %exclude_jupyterlab% output = paste0(script_path, \"/\", sub(\".ipynb\", \".prod.R\", notebook_name)) # %exclude_jupyterlab% cmd1 = paste0(\"jupyter nbconvert --to script --output \", # %exclude_jupyterlab% notebook_prefix, ' ', notebook_path, \"> /dev/null 2>&1 \") # %exclude_jupyterlab% cmd1 # %exclude_jupyterlab% system(cmd1, intern = TRUE) # %exclude_jupyterlab% cmd2 = paste0('cat ', opt_name, ' ', notebook_r, # %exclude_jupyterlab% ' |grep -v exclude_jupyterlab > ', output, ' 2>&1') # %exclude_jupyterlab% cmd2 # %exclude_jupyterlab% system(cmd2, intern = T) # %exclude_jupyterlab% system(paste0(\"rm \", notebook_r)) # %exclude_jupyterlab%","title":"Develop R scripts in the container"},{"location":"docker/","text":"Create images using Docker Docker is not available on FRCE (or any HPC) for a user because docker requires sudo permission. But we can run use docker on our own laptop or VM in which we have been granted the root access. Common command lines used to create and test an image: ## passwd is the same as FRCE password docker login -u ccrsfifx docker build docker tag docker push docker run docker images docker container ls An example of creating customized image for single cell data Command lines for creating and pushing images to Docker hub docker build -t sc-smk-wl -f Dockerfile . docker tag sc-smk-wl:latest ccrsfifx/sc-smk-wl:r1.0.0 docker push ccrsfifx/sc-smk-wl:r1.0.0 To test the image, we can create a container using the image we have built above: docker run -p 8888:8888 -v /Volumes/:/Volumes/ -t -d --name Renv sc-smk-wl:latest Because the image above is built for processing single cell RNA-seq data, we included all the R packages needed: Seurat SingleR celldex scater etc A full list of packages installed can be found here: https://github.com/CCRSF-IFX/SF_biocontainer/blob/main/dockerhub_repos/sc-smk-wl/vr1.0.0/Dockerfile Then we can aunch a Bash shell inside the container: docker exec -it Renv bash In the container, we can lauch jupyter lab and test the R code.","title":"Create image/container"},{"location":"docker/#create-images-using-docker","text":"Docker is not available on FRCE (or any HPC) for a user because docker requires sudo permission. But we can run use docker on our own laptop or VM in which we have been granted the root access. Common command lines used to create and test an image: ## passwd is the same as FRCE password docker login -u ccrsfifx docker build docker tag docker push docker run docker images docker container ls","title":"Create images using Docker"},{"location":"docker/#an-example-of-creating-customized-image-for-single-cell-data","text":"Command lines for creating and pushing images to Docker hub docker build -t sc-smk-wl -f Dockerfile . docker tag sc-smk-wl:latest ccrsfifx/sc-smk-wl:r1.0.0 docker push ccrsfifx/sc-smk-wl:r1.0.0 To test the image, we can create a container using the image we have built above: docker run -p 8888:8888 -v /Volumes/:/Volumes/ -t -d --name Renv sc-smk-wl:latest Because the image above is built for processing single cell RNA-seq data, we included all the R packages needed: Seurat SingleR celldex scater etc A full list of packages installed can be found here: https://github.com/CCRSF-IFX/SF_biocontainer/blob/main/dockerhub_repos/sc-smk-wl/vr1.0.0/Dockerfile Then we can aunch a Bash shell inside the container: docker exec -it Renv bash In the container, we can lauch jupyter lab and test the R code.","title":"An example of creating customized image for single cell data"},{"location":"exec_cmd_sif/","text":"Here we use Seurat analysis as an example. Step 1: Build an image file using Singularity: module load singularity singularity build r_env_v1.0.0.sif docker://ccrsfifx/sc-smk-wl:r1.0.0 The command line above will create a file named r_env_v1.0.0.sif . The suffix sif is short for singularity image file. This image file contains all the required R packages. Using the command line below, you can get the packages installed and the corresponding version information: singularity exec r_env_v1.0.0.sif R -e 'ip = as.data.frame(installed.packages()[,c(1,3:4)]); ip = ip[is.na(ip$Priority),1:2,drop=FALSE];ip' Step 2: Run the command line singularity exec \\ --cleanenv --no-home -B /mnt/ccrsf-static/ -B /mnt/ccrsf-ifx/ -B /scratch/ccrsf_scratch \\ r_env_v1.0.0.sif \\ Rscript sc_seurat.prod.R --genome=hg38 --data.dir=<absolute_path2mtx> --outdir=<outdir> Based on the paths of input and output files, you may want to include additional bindings. To run SingleR script, you can modify the command line above to include the corresponding script and options.","title":"Run command lines using the container outside snakemake pipeline using Singularity"},{"location":"general-questions/","text":"FAQ What is Docker container? A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Docker containers are built from images, which are read-only templates containing the application and its dependencies. Images are defined using a Dockerfile, which specifies the configuration and steps needed to create the image. Once an image is created, it can be instantiated into one or more containers. Each container runs in its own isolated environment but shares the same underlying host operating system with other containers. This allows for efficient resource utilization and easy scaling of applications. Why do we use container? A container is a technology that provides a consistent computational environment and enables reproducibility, scalability, and security when developing NGS bioinformatics analysis pipelines. Containers can increase the bioinformatics team's productivity by automating and simplifying the maintenance of complex bioinformatics resources, as well as facilitate validation, version control, and documentation necessary for clinical laboratory regulatory compliance. What's the difference between container and VM? VMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost \u2014 the computational overhead spent virtualizing hardware for a guest OS to use is substantial. Containers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.","title":"General Questions"},{"location":"general-questions/#faq","text":"","title":"FAQ"},{"location":"general-questions/#what-is-docker-container","text":"A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Docker containers are built from images, which are read-only templates containing the application and its dependencies. Images are defined using a Dockerfile, which specifies the configuration and steps needed to create the image. Once an image is created, it can be instantiated into one or more containers. Each container runs in its own isolated environment but shares the same underlying host operating system with other containers. This allows for efficient resource utilization and easy scaling of applications.","title":"What is Docker container?"},{"location":"general-questions/#why-do-we-use-container","text":"A container is a technology that provides a consistent computational environment and enables reproducibility, scalability, and security when developing NGS bioinformatics analysis pipelines. Containers can increase the bioinformatics team's productivity by automating and simplifying the maintenance of complex bioinformatics resources, as well as facilitate validation, version control, and documentation necessary for clinical laboratory regulatory compliance.","title":"Why do we use container?"},{"location":"general-questions/#whats-the-difference-between-container-and-vm","text":"VMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost \u2014 the computational overhead spent virtualizing hardware for a guest OS to use is substantial. Containers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.","title":"What's the difference between container and VM?"},{"location":"license/","text":"To be added","title":"License"},{"location":"pipeline_containerization/","text":"To use container in the snakemake pipeline, we can define a container for each rule to use: rule seurat_proc: input: h5 = rules.count.output params: fil_mtx = os.path.join(analysis, \"{sample}/outs/filtered_feature_bc_matrix/\"), outdir = os.path.join(analysis, \"{sample}/seurat/\"), log: os.path.join(analysis, \"{sample}/seurat/seurat.log\") output: seur = os.path.join(analysis, \"{sample}/seurat/seur_10x_cluster_object.rds\") container: \"docker://ccrsfifx/sc-smk-wl:r1.0.0\" shell: \"\"\" Rscript {analysis}/workflow/scripts/rna/sc_seurat.prod.R --genome={config.ref} --data.dir={params.fil_mtx} --outdir={params.outdir} > {log} 2>&1 \"\"\"","title":"Build Snakemake pipeline with Singularity"},{"location":"ref/","text":"Reference Singularity User Guide: link Uni.lu High Performance Computing (HPC) Tutorials: link Containers in Bioinformatics: Applications, Practical Considerations, and Best Practices in Molecular Pathology: link Docker for beginners: link","title":"Reference"},{"location":"ref/#reference","text":"Singularity User Guide: link Uni.lu High Performance Computing (HPC) Tutorials: link Containers in Bioinformatics: Applications, Practical Considerations, and Best Practices in Molecular Pathology: link Docker for beginners: link","title":"Reference"},{"location":"snakemake_opt4singularity/","text":"The configuration of snakemake command for singularity is recommended to be set up in profile/slurm/config.v8+.yaml : use-singularity: True singularity-args: ' \"--cleanenv --no-home -B /scratch/ccrsf_scratch -B /mnt/ccrsf-static -B /mnt/ccrsf-ifx -B /mnt/ccrsf-raw -B /mnt/ccrsf-active\" ' The option -B is used to bind the corresponding storage space into the container so that the files from scratch/Qumulo can be accessible from inside the container. --no-home : no host $HOME directory mounting. Auto-binding feature: By default, Singularity automatically binds several directories (e.g.: $HOME , $PWD ) and in particular it binds the home folder of the host. This features simplifies the usage of Singularity for the users, however it can also lead to unexpected behaviours and frustration. For example, if you have different python packages installed in your host home directory and in the container the host packages maybe used instead f the container ones. To avoid this issue, --no-home is used. The --cleanenv option is used to exclude passing EBV variables frin the host into the container. Similar to --no-home , --cleanenv is used to disable Singularity passing host environment variables ( $PATH , $LD_LIBRARY_PATH , etc) to the container.","title":"Snakemake options for containerization"},{"location":"troubleshooting/","text":"Troubleshooting Issue with cache folders and temporary folders To make download of layers for build and pull faster and less redundant, singularity use a caching strategy. By default, the Singularity software will create a set of folders in your $HOME directory for docker layers and metadata, respectively: $HOME/.singularity Singularity also uses some temporary directories to build the squashfs filesystem, so this temp space needs to be large enough to hold the entire resulting Singularity image. By default this happens in /tmp but can be overridden by setting SINGULARITY_TMPDIR to the full path where you want the squashfs temp files to be stored. In many HPC platform, limited space is assigned to $HOME . So to make the pipeline more robust, it is recommended to set SINGULARITY_CACHEDIR to make sure enough space can be used. BoltDB Corruption Errors\uf0c1 The library that SingularityCE uses to retrieve and cache Docker/OCI layers keeps track of them using a single-file database. If your home directory is on a network filesystem which experiences interruptions, or you run out of storage, it is possible for this database to become inconsistent. If you observe error messages that mention github.com/etcd-io/bbolt when trying to run SingularityCE, then you should remove the database file: rm ~/.local/share/containers/cache/blob-info-cache-v1.boltdb Here are the discussions useful for this issue: https://github.com/apptainer/singularity/issues/5329#issuecomment-637595826 https://docs.sylabs.io/guides/main/user-guide/build_env.html#boltdb-corruption-errors https://github.com/apptainer/singularity/issues/5329#issuecomment-1062000005 Issue with NIH VPN When the program inside a container needs to connect to a database via API or download some data, the error message below might be threwn out: SSL certificate problem: self signed certificate in certificate chain This is related to VPN setting. Disabling VPN solved the issue. Detailed reason not sure. Issue with internal connection","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#issue-with-cache-folders-and-temporary-folders","text":"To make download of layers for build and pull faster and less redundant, singularity use a caching strategy. By default, the Singularity software will create a set of folders in your $HOME directory for docker layers and metadata, respectively: $HOME/.singularity Singularity also uses some temporary directories to build the squashfs filesystem, so this temp space needs to be large enough to hold the entire resulting Singularity image. By default this happens in /tmp but can be overridden by setting SINGULARITY_TMPDIR to the full path where you want the squashfs temp files to be stored. In many HPC platform, limited space is assigned to $HOME . So to make the pipeline more robust, it is recommended to set SINGULARITY_CACHEDIR to make sure enough space can be used.","title":"Issue with cache folders and temporary folders"},{"location":"troubleshooting/#boltdb-corruption-errors","text":"The library that SingularityCE uses to retrieve and cache Docker/OCI layers keeps track of them using a single-file database. If your home directory is on a network filesystem which experiences interruptions, or you run out of storage, it is possible for this database to become inconsistent. If you observe error messages that mention github.com/etcd-io/bbolt when trying to run SingularityCE, then you should remove the database file: rm ~/.local/share/containers/cache/blob-info-cache-v1.boltdb Here are the discussions useful for this issue: https://github.com/apptainer/singularity/issues/5329#issuecomment-637595826 https://docs.sylabs.io/guides/main/user-guide/build_env.html#boltdb-corruption-errors https://github.com/apptainer/singularity/issues/5329#issuecomment-1062000005","title":"BoltDB Corruption Errors\uf0c1"},{"location":"troubleshooting/#issue-with-nih-vpn","text":"When the program inside a container needs to connect to a database via API or download some data, the error message below might be threwn out: SSL certificate problem: self signed certificate in certificate chain This is related to VPN setting. Disabling VPN solved the issue. Detailed reason not sure.","title":"Issue with NIH VPN"},{"location":"troubleshooting/#issue-with-internal-connection","text":"","title":"Issue with internal connection"}]}